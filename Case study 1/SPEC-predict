# from google.colab import drive
# drive.mount('/content/drive/')
# %cd /content/drive/My Drive/Colab Notebooks/
import numpy as np
import pandas as pd
import torch as t
import matplotlib.pyplot as plt

if t.cuda.is_available():
    device = t.device('cuda')
else:
    device = t.device('cpu')
print('Device: ' + str(device))


class SpecDataset(t.utils.data.Dataset):
    def __init__(self, src_df):
        xdf = src_df.drop(columns='run time').values
        ydf = src_df['run time'].values.reshape(-1, 1)  # 2-D required
        self.x_data = t.tensor(xdf, dtype=t.float32).to(device)
        self.y_data = t.tensor(ydf, dtype=t.float32).to(device)

    def __len__(self):
        return len(self.x_data)

    def __getitem__(self, idx):
        preds = self.x_data[idx]
        runtime = self.y_data[idx]
        return preds, runtime  # tuple of two matrices


class Net(t.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.hid1 = t.nn.Linear(49, 100)  # 49-(100-100-100)-1
        self.hid2 = t.nn.Linear(100, 100)
        self.hid3 = t.nn.Linear(100, 100)
        self.oupt = t.nn.Linear(100, 1)

    def forward(self, x):
        z = t.tanh(self.hid1(x))
        z = t.tanh(self.hid2(z))
        z = t.tanh(self.hid3(z))
        z = self.oupt(z)  # no activation
        return z


class LR(t.nn.Module):
    def __init__(self):
        super(LR, self).__init__()
        self.linear = t.nn.Linear(49, 1)

    def forward(self, x):
        z = self.linear(x)  # no activation
        return z


def main():
    bat_size = 20
    num_epochs = 50
    num_reps = 10
    ep_log_interval = 50
    lrn_rate = 0.001
    reg_lambda = 0
    k = 3  # top-k accuracy

    df = pd.read_csv('sim.csv')
    archname = ['sandy bridge', 'ivy bridge', 'haswell', 'broadwell', 'skylake', 'average']
    archmae = [0] * 6  # for storing average DNN model MAE
    archsd = [0] * 6  # for storing average DNN model SD
    archacc = [0] * 6  # for storing DNN model accuracy
    lrarchmae = [0] * 6  # for storing average LR model MAE
    lrarchsd = [0] * 6  # for storing average LR model SD
    lrarchacc = [0] * 6  # for storing LR model accuracy
    # loop over 5 micro architectures
    for uarch in range(5):
        print("Uarch : " + archname[uarch])
        uarch_df = df[df['uarch'] == uarch]
        nuarch_df = df[df['uarch'] != uarch]
        n_correct = 0
        n_wrong = 0
        lrn_correct = 0
        lrn_wrong = 0
        mae = np.zeros(num_reps)  # array of mae of each repetition DNN
        lrmae = np.zeros(num_reps)  # array of mae of each repetition LR
        # 20 repetitions for each micro architecture
        for rep in range(1, num_reps + 1):
            # gather randomly 10% data of uarch for test_ds and rest of all as train_ds
            print("\tRepetition : %2d" % rep)
            test_df = uarch_df.sample(frac=0.1, random_state=rep)
            traindf = uarch_df.drop(test_df.index)
            train_df = pd.concat([nuarch_df, traindf])
            test_df.reset_index(drop=True, inplace=True)
            min_rt_idx = test_df['run time'].idxmin()  # minimum runtime index in test dataset

            train_ds = SpecDataset(train_df)
            test_ds = SpecDataset(test_df)
            train_ldr = t.utils.data.DataLoader(train_ds, batch_size=bat_size, shuffle=True)
            test_ldr = t.utils.data.DataLoader(test_ds, batch_size=bat_size)

            net = Net().to(device)
            loss_func = t.nn.L1Loss()
            optimizer = t.optim.RMSprop(net.parameters(), lr=lrn_rate)
            train_loss = []
            val_loss = []
            lrm = LR().to(device)
            lroptimizer = t.optim.RMSprop(lrm.parameters(), lr=lrn_rate)
            lrtrain_loss = []
            lrval_loss = []

            for epoch in range(1, num_epochs + 1):
                t.manual_seed(epoch)  # recovery reproducibility
                net.train()  # set mode
                epoch_loss = 0  # for one full epoch
                lrm.train()
                lrepoch_loss = 0

                for X, Y in train_ldr:
                    oupt = net(X)  # predicted prices
                    loss_val = loss_func(oupt, Y)  # avg per item in batch
                    loss_val += reg_lambda * sum(p.abs().sum() for p in net.parameters())  # L1 regularization
                    epoch_loss += loss_val.item()  # accumulate avgs
                    optimizer.zero_grad()  # prepare gradients
                    loss_val.backward()  # compute gradients
                    optimizer.step()  # update wts

                    lroupt = lrm(X)  # predicted prices
                    lrloss_val = loss_func(lroupt, Y)  # avg per item in batch
                    lrepoch_loss += lrloss_val.item()  # accumulate avgs
                    lrloss_val += reg_lambda * sum(p.abs().sum() for p in lrm.parameters())  # L1 regularization
                    lroptimizer.zero_grad()  # prepare gradients
                    lrloss_val.backward()  # compute gradients
                    lroptimizer.step()  # update wts
                epoch_loss /= len(train_ldr)
                train_loss += [epoch_loss]
                lrepoch_loss /= len(train_ldr)
                lrtrain_loss += [lrepoch_loss]

                net.eval()
                epoch_vloss = 0
                lrm.eval()
                lrepoch_vloss = 0
                for X, Y in test_ldr:
                    with t.no_grad():
                        oupt = net(X)
                        epoch_vloss += loss_func(oupt, Y).item()
                        lroupt = lrm(X)
                        lrepoch_vloss += loss_func(lroupt, Y).item()

                epoch_vloss /= len(test_ldr)
                val_loss += [epoch_vloss]
                lrepoch_vloss /= len(test_ldr)
                lrval_loss += [lrepoch_vloss]

                if epoch % ep_log_interval == 0 or epoch == 1:
                    print("\t\tEpoch = %4d   loss = %0.4f" % (epoch, epoch_loss))
                    print("\t\t               val loss = %0.4f" % epoch_vloss)
                    print("\t\t           LR loss = %0.4f" % lrepoch_loss)
                    print("\t\t           LR  val loss = %0.4f" % lrepoch_vloss)

            # path = f"./Case1/SpecLog/Uarch{uarch}_Rep{rep}.pth"
            # info_dict = {
            #    'repetition': rep,
            #    'net_state': net.state_dict(),
            #    'optimizer_state': optimizer.state_dict()
            # }
            # t.save(info_dict, path)
            if rep == 1:
                plt.plot(range(1, num_epochs + 1), train_loss, 'g', label='Training loss')
                plt.plot(range(1, num_epochs + 1), val_loss, 'b', label='Validation loss')
                plt.xlabel('Epochs')
                plt.ylabel('Loss')
                plt.title('Training and Validation loss using DNN')
                plt.legend()
                plt.savefig(archname[uarch] + " DNN.png")
                plt.close()

                plt.plot(range(1, num_epochs + 1), lrtrain_loss, 'g', label='Training loss')
                plt.plot(range(1, num_epochs + 1), lrval_loss, 'b', label='Validation loss')
                plt.xlabel('Epochs')
                plt.ylabel('Loss')
                plt.title('Training and Validation loss using LR')
                plt.legend()
                plt.savefig(archname[uarch] + " LR.png")
                plt.close()

            net.eval()
            preds = []
            lrm.eval()
            lrpreds = []
            for X, Y in test_ldr:
                with t.no_grad():
                    oupt = net(X)
                    preds += oupt.tolist()  # computed runtime
                    mae[rep - 1] += loss_func(oupt, Y).item()
                    lroupt = lrm(X)
                    lrpreds += lroupt.tolist()  # computed runtime
                    lrmae[rep - 1] += loss_func(lroupt, Y).item()

            mae[rep - 1] /= len(test_ldr)
            print("\tMAE of repetition %2d DNN predictions = %0.4f" % (rep, mae[rep - 1]))
            min_rt_pred = preds[min_rt_idx]  # predicted runtime value for SKU which has minimum runtime
            preds.sort()
            print("\tRank of best SKU in test SKU DNN predictions = %4d in %4d" % (preds.index(min_rt_pred), len(preds)))
            if preds.index(min_rt_pred) < k:  # min runtime is among top k predicted min runtimes
                n_correct += 1
            else:
                n_wrong += 1

            lrmae[rep - 1] /= len(test_ldr)
            print("\tMAE of repetition %2d LR predictions = %0.4f" % (rep, lrmae[rep - 1]))
            lrmin_rt_pred = lrpreds[min_rt_idx]  # predicted runtime value for SKU which has minimum runtime
            lrpreds.sort()
            print("\tRank of best SKU in test SKU LR predictions = %4d in %4d" % (
                lrpreds.index(lrmin_rt_pred), len(lrpreds)))
            if lrpreds.index(lrmin_rt_pred) < k:  # min runtime is among top k predicted min runtimes
                lrn_correct += 1
            else:
                lrn_wrong += 1

        archmae[uarch] = mae.mean()
        archsd[uarch] = mae.std()
        archacc[uarch] = (n_correct * 1.0) / (n_correct + n_wrong)  # accuracy of this uarch
        print("Average MAE, SD and accuracy of %s DNN predictions = %0.4f %0.4f %0.4f" % (archname[uarch], archmae[uarch], archsd[uarch], archacc[uarch]))

        lrarchmae[uarch] = lrmae.mean()
        lrarchsd[uarch] = lrmae.std()
        lrarchacc[uarch] = (lrn_correct * 1.0) / (lrn_correct + lrn_wrong)  # accuracy of this uarch
        print("Average MAE, SD and accuracy of %s LR predictions = %0.4f %0.4f %0.4f" % (archname[uarch], lrarchmae[uarch], lrarchsd[uarch], lrarchacc[uarch]))

    archmae[5] = sum(archmae[:5]) / 5
    archsd[5] = sum(archsd[:5]) / 5
    archacc[5] = sum(archacc[:5]) / 5
    lrarchmae[5] = sum(lrarchmae[:5]) / 5
    lrarchsd[5] = sum(lrarchsd[:5]) / 5
    lrarchacc[5] = sum(lrarchacc[:5]) / 5
    # creating the bar plots
    barWidth = 0.33
    fig = plt.subplots(figsize=(5, 3))
    br1 = np.arange(6)
    br2 = [x + barWidth for x in br1]
    plt.bar(br1, archmae, yerr=archsd, color='c', width=barWidth, label='DNN', capsize=3, ecolor='b')
    plt.bar(br2, lrarchmae, yerr=lrarchsd, color='y', width=barWidth, label='LR', capsize=3, ecolor='b')
    plt.ylabel('MAE', fontweight='bold', fontsize=10)
    plt.title("SPEC")
    plt.xticks([r + barWidth/2 for r in range(6)], archname, fontsize=10, rotation=20)
    plt.yticks(fontsize=10)
    plt.grid(axis='y', ls=(0, (1, 4)))
    plt.legend()
    plt.tight_layout()
    plt.savefig("SPEC MAE.png")
    plt.close()

    fig = plt.subplots(figsize=(5, 3))
    plt.bar(br1, archacc, color='g', width=barWidth, label='DNN')
    plt.bar(br2, lrarchacc, color='b', width=barWidth, label='LR')
    plt.ylabel('Accuracy', fontweight='bold', fontsize=10)
    plt.xticks([r + barWidth / 2 for r in range(6)], archname, fontsize=10, rotation=20)
    plt.yticks(fontsize=10)
    plt.legend()
    plt.title("SPEC")
    plt.grid(axis='y')
    plt.tight_layout()
    plt.savefig("SPEC accuracy.png")
    plt.close()


if __name__ == "__main__":
    main()
